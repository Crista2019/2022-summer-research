import matplotlib
import csv
import numpy as np
import sklearn.preprocessing as prep
import torch
import math
import torch.nn as nn
import torch.nn.functional as F

"""
Hc3-cell labels:

id integer, -- Id used to match original row number in MatLab PyrIntMap.Map matrix

topdir string, -- top level directory containing data

animal string, -- name of animal

ele integer, -- electrode number

clu integer, -- ID # in cluster files: result of spike sorting steps -> spikes most likely generated by the same neuron placed into a category (cluster), which is assigned a non-negative integer cluster number 

region string, -- brain region

nexciting integer, -- number of cells this cell monosynaptically excited

ninhibiting integer, -- number of cells this cell monosynaptically inhibited

exciting integer, -- physiologically identified exciting cells based on CCG analysis

inhibiting integer, -- physiologically identified inhibiting cells based on CCG analysis (Detailed method can be found in Mizuseki Sirota Pastalkova and Buzsaki., 2009 Neuron paper.) 

excited integer, -- based on cross-correlogram analysis, the cell is monosynaptically excited by other cells

inhibited integer, -- based on cross-correlogram analysis, the cell is monosynaptically inhibited by other cells

fireRate real, -- meanISI=mean(bootstrp(100,'mean',ISI)); fireRate = SampleRate/MeanISI; ISI is interspike intervals.

totalFireRate real, -- num of spikes divided by total recording length for a period with a high response rate

cellType string -- ''p'=pyramidal, 'i'=interneuron, 'n'=not identified as pyramidal or interneuron

"""

cell_data = open('hc3-cell.csv')

header = ['animal', 'ele', 'clu', 'region', 'nexciting', 'ninhibiting', 'exciting', 'inhibiting', 'excited', 'inhibited', 'fireRate', 'totalFireRate', 'cellType']

csvreader = csv.reader(cell_data)

def convert_to_one_hot(val, n, categories=None):
	"""
	inputs:
		- val: a single string representing ONE element of categorical data to be converted
		- n: an int representing the number of classes being mapped total across *all* data (this can be larger than the categories size, if you want to pad the encoding with zeros)
		- categories: tuple conatining all categories of data, whose indices can be used to map the excoding to ints for the one_hot func
					e.g. ('data1', 'data2', 'data3', ...)
	outputs:
		- a size n PyTorch tensor containing a list of all 0. (floats) and a single 1. which maps the input to its distinct category
	
	the purpose of this function is to be called at each iteration of reading the raw csv data 
	(or iterating through the rows of data later) and return the corresponding binary one hot encoding 
	"""
	if not categories:	
		# if no mapping is provided, the data is ordered ints (therefore the mapping is inferred)
		if not type(val) == int:
			raise Exception("Mapping cannot be inferred for string data. Please provide categories as well.")
		return F.one_hot(torch.tensor([val]), num_classes=n).float()
	else:
		# if category is a string, convert to an int then one hot encode
		if not val in categories:
			raise Exception("Invalid category entered, cannot one-hot encode")

		if len(categories) > n:
			raise Exception("Category list must be of size n or smaller")

		int_encoding = categories.index(val)
		return F.one_hot(torch.tensor([int_encoding]), num_classes=n).float()

def num_to_tensor(num):
    """
    simple function that takes in an string that represents an int or float and returns a size 1 tensor of floats
    """
    return torch.tensor([[float(num)]], dtype=float)

# feature representation: 
# 0: animal -> one hot encoding (categories but no numerical ordering); 11 distinct animals
# 1: ele -> one hot encoding; 16
# 2: clu -> num of types of spikes found (keep as int)
# 3: region -> one hot encoding; 9 regions
# 4, 5, 6, 7, 8, 9: all cell number data -> keep as ints since these are counts
# 10, 11: firing rates -> keep as floats, but use z score
# 12: cellType -> one hot encoding (00, 01, 10 for three cell types)

rows = []
output = []
indices_to_standardize = [] # tracks the indices of the rows with data to standardize with z-score (initially float data)
all_indices_tracked = False

for row in csvreader:
    transformed_data = [] # initialize variable (will be replaced by tensor then concatenated)

    # TODO: tranpose the one hot encoding and concat with the other data so we add dimensions to the input but end up with a tensor of floats
    # then standardize the floats after the fact
    
    row = row[2:] # remove unhelpful label data

    if 'NaN' in row:
        # filter out rows with incomplete data
        continue
    
    # animal (string) -> OHE
    animal_ohe = convert_to_one_hot(row[0], 11, ('ec012','ec013','ec014','ec016','f01_m','g01_m','gor01','i01_m','j01_m','pin01','vvp01'))
    transformed_data = torch.transpose(animal_ohe, 0, 1)

    # electrode (int) -> OHE
    electrode_ohe = convert_to_one_hot(int(row[1])-1, 16)
    transformed_data = torch.cat((transformed_data, torch.transpose(electrode_ohe,0,1)),0)
    
    # CLU -> OHE
    cluster_num = convert_to_one_hot(int(row[2])-1, 32)
    transformed_data = torch.cat((transformed_data, torch.transpose(cluster_num,0,1)),0)

    # region (string) -> OHE
    region_ohe = convert_to_one_hot(row[3], 9, ('EC2','EC3','EC4','EC5','EC?','CA1','CA3','DG','Unknown')) 
    transformed_data = torch.cat((transformed_data, torch.transpose(region_ohe,0,1)),0)

    # cell number values -> floats
    nexciting = num_to_tensor(row[4]) 
    transformed_data = torch.cat((transformed_data, nexciting),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    ninhibiting = num_to_tensor(row[5])
    transformed_data = torch.cat((transformed_data, ninhibiting),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    exciting = num_to_tensor(row[6])
    transformed_data = torch.cat((transformed_data, exciting),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    inhibiting = num_to_tensor(row[7])
    transformed_data = torch.cat((transformed_data, inhibiting),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    excited = num_to_tensor(row[8])
    transformed_data = torch.cat((transformed_data, excited),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    inhibited = num_to_tensor(row[9])
    transformed_data = torch.cat((transformed_data, inhibited),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    fireRate = num_to_tensor(row[10])
    transformed_data = torch.cat((transformed_data, fireRate),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    tot_fireRate = num_to_tensor(row[11])
    transformed_data = torch.cat((transformed_data, tot_fireRate),0)
    if not all_indices_tracked: indices_to_standardize.append(transformed_data.shape[0]-1)
    
    # cellType -> OHE
    cell_ohe = convert_to_one_hot(row[12], 3, ('i','p','n'))
    # replace one hot encoding with index of encoded value for NLLLoss: e.g. [0, 0, 1, 0] -> [2]
    # https://stackoverflow.com/questions/66635987/how-to-solve-this-pytorch-runtimeerror-1d-target-tensor-expected-multi-target
    cell_ohe = cell_ohe[0].nonzero()[0]
    output.append(cell_ohe)

    all_indices_tracked = True
    rows.append(transformed_data)

cell_data.close()

input_data = torch.hstack(rows) 

# classification for cell type
output_label = torch.vstack(output).float() # each row contains the one hot encoding for the type of cell per subject


# print('interneuron count:', sum(output_label[-3].numpy(),1))
# print('pyrimidal count:', sum(output_label[-2].numpy(),1))
# print('not defined:', sum(output_label[-1].numpy(),1))

# interneuron count: 1132.0
# pyrimidal count: 6100.0
# not defined: 504.0

# iterate over the float (not one hot encoded data) and standardize using z-score
# z = (x - u) / s
for i in indices_to_standardize:

    # convert to numpy array to perform transformation
    altered_row = input_data[i].numpy().reshape(1,-1).transpose()

    # standard scalar in order to convert to z score
    scale = prep.StandardScaler()
    scale.fit(altered_row)
    altered_row = scale.transform(altered_row).transpose()

    # redefine the original data as a tensor of the z-score data
    input_data[i] = torch.tensor(altered_row, dtype=float)

input_data = input_data.transpose(-1,0) # where each row is a different subject and each column is an input feature

# start of the neural network

input_dims = input_data.shape[1]
output_dims = output_label.shape[1]
# print(input_data)
# print(output_label)
# print(input_dims, output_dims) # 76, 3
# print(input_data.dtype)
# print(output_label.dtype)

model = torch.nn.Sequential(
    torch.nn.Linear(input_dims,100),
    # torch.nn.LayerNorm(10),
    torch.nn.ReLU(),
    torch.nn.Linear(100,5),
    # torch.nn.LayerNorm(5),
    torch.nn.ReLU(),
    torch.nn.Linear(5,output_dims),
    # torch.nn.LayerNorm(output_dims),
    # torch.nn.ReLU(),
    torch.nn.Softmax(dim=1) # sigmoid?
)

loss_fn = torch.nn.MSELoss() 

learning_rate = 1e-6
for t in range(2000):
    y_pred = model(input_data.float())
    loss = loss_fn(y_pred, output_label)
    # break

    # loss = loss_fn(y_pred.transpose(-1,0), output_label.transpose(-1,0))
    if t % 100 == 99:
        print(y_pred.transpose(-1,0))
        print(output_label.transpose(-1,0))
        print(t, loss.item())
        # print('--------')
    model.zero_grad() # Zero the gradients before running the backward pass.
    loss.backward()
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad
# print('linear_layer', model[0])