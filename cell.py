import csv
import math

import matplotlib.pyplot as plt
import numpy as np
import sklearn.discriminant_analysis as discrim
import sklearn.metrics as met
import sklearn.model_selection as ms
import sklearn.preprocessing as prep
import torch
import torch.nn.functional as F
import torch.utils.data as data_utils
from sklearn.utils import class_weight

from tayloranalysis import TaylorAnalysis

"""
Hc3-cell labels:

id integer, -- Id used to match original row number in MatLab PyrIntMap.Map matrix

topdir string, -- top level directory containing data

animal string, -- name of animal

ele integer, -- electrode number

clu integer, -- ID # in cluster files: result of spike sorting steps -> spikes most likely generated by the same neuron placed into a category (cluster), which is assigned a non-negative integer cluster number

region string, -- brain region

nexciting integer, -- number of cells this cell monosynaptically excited

ninhibiting integer, -- number of cells this cell monosynaptically inhibited

exciting integer, -- physiologically identified exciting cells based on CCG analysis

inhibiting integer, -- physiologically identified inhibiting cells based on CCG analysis (Detailed method can be found in Mizuseki Sirota Pastalkova and Buzsaki., 2009 Neuron paper.)

excited integer, -- based on cross-correlogram analysis, the cell is monosynaptically excited by other cells

inhibited integer, -- based on cross-correlogram analysis, the cell is monosynaptically inhibited by other cells

fireRate real, -- meanISI=mean(bootstrp(100,'mean',ISI)); fireRate = SampleRate/MeanISI; ISI is interspike intervals.

totalFireRate real, -- num of spikes divided by total recording length for a period with a high response rate

cellType string -- ''p'=pyramidal, 'i'=interneuron, 'n'=not identified as pyramidal or interneuron
"""

cell_data = open("hc3-cell.csv")

header = [
    "animal",
    "ele",
    "clu",
    "region",
    "nexciting",
    "ninhibiting",
    "exciting",
    "inhibiting",
    "excited",
    "inhibited",
    "fireRate",
    "totalFireRate",
    "cellType",
]

csvreader = csv.reader(cell_data)

def convert_to_one_hot(val, n, categories=None):
    """
    inputs:
    - val: a single string representing ONE element of categorical data to be converted
    - n: an int representing the number of classes being mapped total across *all* data (this can be larger than the categories size, if you want to pad the encoding with zeros)
    - categories: tuple conatining all categories of data, whose indices can be used to map the excoding to ints for the one_hot func
                            e.g. ('data1', 'data2', 'data3', ...)
    outputs:
    - a size n PyTorch tensor containing a list of all 0. (floats) and a single 1. which maps the input to its distinct category

    the purpose of this function is to be called at each iteration of reading the raw csv data
    (or iterating through the rows of data later) and return the corresponding binary one hot encoding
    """
    if not categories:
        # if no mapping is provided, the data is ordered ints (therefore the mapping is inferred)
        if not type(val) == int:
            raise Exception("Mapping cannot be inferred for string data. Please provide categories as well.")
        return F.one_hot(torch.tensor([val]), num_classes=n).float()
    else:
        # if category is a string, convert to an int then one hot encode
        if not val in categories:
            raise Exception("Invalid category entered, cannot one-hot encode")

        if len(categories) > n:
            raise Exception("Category list must be of size n or smaller")

        int_encoding = categories.index(val)
        return F.one_hot(torch.tensor([int_encoding]), num_classes=n).float()


def num_to_tensor(num):
    """
    simple function that takes in an string that represents an int or float and returns a size 1 tensor of floats
    """
    return torch.tensor([[float(num)]], dtype=float)


# feature representation:
# 0: animal -> one hot encoding (categories but no numerical ordering); 11 distinct animals
# 1: ele -> one hot encoding; 16
# 2: clu -> num of types of spikes found (keep as int)
# 3: region -> one hot encoding; 9 regions
# 4, 5, 6, 7, 8, 9: all cell number data -> keep as ints since these are counts
# 10, 11: firing rates -> keep as floats, but use z score
# 12: cellType -> one hot encoding (00, 01, 10 for three cell types)

rows = []
output = []
indices_to_standardize = []  # tracks the indices of the rows with data to standardize with z-score (initially float data)
all_indices_tracked = False

for row in csvreader:
    transformed_data = []  # initialize variable (will be replaced by tensor then concatenated)
    label = []

    row = row[2:]  # remove unhelpful label data

    if "NaN" in row:
        # filter out rows with incomplete data
        continue

    # animal (string) -> OHE
    animal_ohe = convert_to_one_hot(
        row[0], 11, ("ec012", "ec013", "ec014", "ec016", "f01_m", "g01_m", "gor01", "i01_m", "j01_m", "pin01", "vvp01")
    )
    transformed_data = torch.transpose(animal_ohe, 0, 1)

    # electrode (int) -> OHE
    electrode_ohe = convert_to_one_hot(int(row[1]) - 1, 16)
    transformed_data = torch.cat((transformed_data, torch.transpose(electrode_ohe, 0, 1)), 0)

    # CLU -> OHE
    cluster_num = convert_to_one_hot(int(row[2]) - 1, 32)
    transformed_data = torch.cat((transformed_data, torch.transpose(cluster_num, 0, 1)), 0)

    # region (string) -> OHE
    region_ohe = convert_to_one_hot(row[3], 9, ("EC2", "EC3", "EC4", "EC5", "EC?", "CA1", "CA3", "DG", "Unknown"))
    transformed_data = torch.cat((transformed_data, torch.transpose(region_ohe, 0, 1)), 0)

    # cell number values -> floats
    nexciting = num_to_tensor(row[4])
    transformed_data = torch.cat((transformed_data, nexciting), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    ninhibiting = num_to_tensor(row[5])
    transformed_data = torch.cat((transformed_data, ninhibiting), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    exciting = num_to_tensor(row[6])
    transformed_data = torch.cat((transformed_data, exciting), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    inhibiting = num_to_tensor(row[7])
    transformed_data = torch.cat((transformed_data, inhibiting), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    excited = num_to_tensor(row[8])
    transformed_data = torch.cat((transformed_data, excited), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    inhibited = num_to_tensor(row[9])
    transformed_data = torch.cat((transformed_data, inhibited), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    fireRate = num_to_tensor(row[10])
    transformed_data = torch.cat((transformed_data, fireRate), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    tot_fireRate = num_to_tensor(row[11])
    transformed_data = torch.cat((transformed_data, tot_fireRate), 0)
    if not all_indices_tracked:
        indices_to_standardize.append(transformed_data.shape[0] - 1)

    # cellType -> OHE
    cell_ohe = convert_to_one_hot(row[12], 3, ("i", "p", "n"))
    label = torch.transpose(cell_ohe, 0, 1)

    all_indices_tracked = True
    rows.append(transformed_data)
    output.append(label)

cell_data.close()

input_data = torch.hstack(rows)

# classification for cell type
output_label = torch.hstack(output).float().transpose(-1, 0)  # each row contains the one hot encoding for the type of cell per subject

# print('interneuron count:', sum(output_label.transpose(-1,0)[-3].numpy(),1))
# print('pyrimidal count:', sum(output_label.transpose(-1,0)[-2].numpy(),1))
# print('not defined:', sum(output_label.transpose(-1,0)[-1].numpy(),1))

# interneuron count: 1133.0
# pyrimidal count: 6096.0
# not defined: 495.0

# iterate over the float (not one hot encoded data) and standardize using z-score
# z = (x - u) / s
for i in indices_to_standardize:

    # convert to numpy array to perform transformation
    altered_row = input_data[i].numpy().reshape(1, -1).transpose()

    # standard scalar in order to convert to z score
    scale = prep.StandardScaler()
    scale.fit(altered_row)
    altered_row = scale.transform(altered_row).transpose()

    # redefine the original data as a tensor of the z-score data
    input_data[i] = torch.tensor(altered_row, dtype=float)

input_data = input_data.transpose(-1, 0)  # where each row is a different subject and each column is an input feature

# divide into 70% train and 30% train
x_train, x_test, y_train, y_test = ms.train_test_split(input_data, output_label, test_size=0.3, shuffle=False)

# classweights                                     # y_train: (N, 3)
train_y_ints = np.argmax(y_train, axis=1).numpy()  # (N, )
train_classes = np.unique(train_y_ints)  #         # (3, )
test_y_ints = np.argmax(y_test, axis=1).numpy()
test_classes = np.unique(test_y_ints)


train_weights = class_weight.compute_class_weight(
    class_weight="balanced",
    classes=train_classes,
    y=train_y_ints,
)  # (3, )

tr_weights = np.zeros_like(y_train.T[0]).astype(float)  # (N, )
for _weight, _label in zip(train_weights, train_classes):
    tr_weights += (train_y_ints == _label).astype(float) * _weight

train_sampler = data_utils.WeightedRandomSampler(
    weights=tr_weights,  # (N, )
    num_samples=tr_weights.shape[0],  # N
    replacement=True,
)


# combine the train data set
train_data = []
for i in range(len(x_train)):
    train_data.append([x_train[i], y_train[i]])

test_weights = class_weight.compute_class_weight(
    class_weight="balanced",
    classes=test_classes,
    y=test_y_ints,
)  # (3, )

ts_weights = np.zeros_like(y_test.T[0]).astype(float)  # (N, )
for _weight, _label in zip(test_weights, test_classes):
    ts_weights += (test_y_ints == _label).astype(float) * _weight

test_sampler = data_utils.WeightedRandomSampler(
    weights=ts_weights,  # (N, )
    num_samples=ts_weights.shape[0],  # N
    replacement=True,
)

# combine the test data set
test_data = []
for i in range(len(x_test)):
    test_data.append([x_test[i], y_test[i]])

train_loader = data_utils.DataLoader(train_data, batch_size=1000, shuffle=False, sampler=train_sampler)
test_loader = data_utils.DataLoader(test_data, batch_size=len(ts_weights), shuffle=False)

# start of the neural network

input_dims = input_data.shape[1]
output_dims = output_label.shape[1]
# print(input_data)
# print(output_label)
# print(input_dims, output_dims) # 76, 3
# print(input_data.dtype)
# print(output_label.dtype)

train_losses = []
eval_losses = []

class Network(torch.nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.fc = torch.nn.Linear(input_dims, 100)
        self.fc1 = torch.nn.Linear(100, 100)
        self.fc2 = torch.nn.Linear(100, output_dims)
        self.soft = torch.nn.Softmax(dim=1)

    def forward(self, y):
        y = F.relu(self.fc(y))
        y = F.relu(self.fc1(y))
        y = F.relu(self.fc2(y))
        y = self.soft(y)
        return y

models = Network()
if torch.cuda.is_available():
    models = models.cuda()

# hyperparams
learning_rate = 1e-3
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(models.parameters(), lr=learning_rate, momentum=0.9)
epoch = 10 #3500
# checkpoint_loss = False

def save_model(epochs, model, optimizer, criterion):
    """
    function to save the trained model to disk
    """
    print(f"Saving best model...")
    torch.save({
        'epoch': epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': criterion,
        }, 'outputs/final_model.pth')

# Taylor Coefficient Analysis yaay!
models = TaylorAnalysis(models)

def list_variable_names(root_word, n):
    """
    takes in a list of numbers and returns a list of variable names with the correct range
    """
    return ['{}{}'.format(root_word,i+1) for i in range(n)]

# Determine names for all the input features for tc graphing
animal_vars = list_variable_names('animal',11)
ele_vars = list_variable_names('electrode',16)
clu_vars = list_variable_names('cluster',32)
region_vars = list_variable_names('region',9)
cell_vars = ['nexciting','ninhibiting','exciting','inhibiting','excited','inhibited','fireRate','totFireRate']
all_vars = animal_vars+ele_vars+clu_vars+region_vars+cell_vars

models.setup_tc_checkpoints(
    number_of_variables_in_data = input_dims,                   # dimension of your model input
    considered_variables_idx = range(input_dims),         # variables to be tracked
    variable_names = all_vars,                                  # their representative names (plotting)
    derivation_order=1,                                         # calculates derivation up to 3, including 3
    eval_nodes='all',                                           # computes TCs based on specified output node(s)
    eval_only_max_node=False                                    # compute TCs based on the output node with the highest value
)

# initial condition for early stopping
loss_not_improved = 0 # count of how many epochs have passed without the loss improving
prev_loss = float('inf')

for i in range(epoch):
    print(f"Epoch: {i}", end="\r")

    # train
    models.train()
    batch_losses = []
    for idx, batch in enumerate(train_loader):
        data, label = batch
        if torch.cuda.is_available():
            data, label = data.cuda(), label.cuda()
        optimizer.zero_grad()
        targets = models(data.float())
        loss = loss_fn(targets, label)
        loss.backward()
        optimizer.step()
        loss_item = loss.detach().item()
        batch_losses.append(loss_item)

        # save current taylorcoefficients
        models.tc_checkpoint(data.float(), epoch=i) # x_train shape is as usual: (batch, features)

    train_losses.append(np.array(batch_losses).mean())
    
    # evaluation
    epoch_test = []
    models.eval()
    for idx, batch in enumerate(test_loader):
        data, label = batch
        if torch.cuda.is_available():
            data, label = data.cuda(), label.cuda()
        targets = models(data.float())
        loss = loss_fn(targets, label)
        loss_item = loss.detach().item()
        eval_losses.append(loss_item)

    # early stopping condition
    if prev_loss < eval_losses[-1]:
        loss_not_improved += 1
    else:
        loss_not_improved = 0
    prev_loss = eval_losses[-1]

    # stop the training if loss hasn't improved on the evaluation set for 50 epochs
    if loss_not_improved >= 50:
        break 
    
    # flag if no model has been saved yet (uncomment only on first run)
    # if not checkpoint_loss:
    #     save_model(epoch, models, optimizer, loss_item)
    
    # save model with best loss
    if not torch.cuda.is_available():
        checkpoint = torch.load('outputs/final_model.pth', map_location=torch.device('cpu'))
    else:
        # cuda by default
        checkpoint = torch.load('outputs/final_model.pth')
    checkpoint_loss = checkpoint['loss']
    if loss < checkpoint_loss:
        print('Better loss found!', loss_item)
        save_model(epoch, models, optimizer, loss_item)


# plot the losses
fig1, ax1 = plt.subplots()
ax1.set_title("Training Loss")
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Loss")
ax1.scatter(np.arange(len(train_losses)), train_losses, label="train loss")
ax1.scatter(np.arange(len(eval_losses)), eval_losses, label="evaluation loss")
ax1.legend()
plt.tight_layout()
plt.savefig("outputs/loss.png")
plt.show()


def decipher_class(input_data):
    # input: a dataset of type float tensor in form: [[0.,0.,1.],[0.00002,1.00,0.008],...]
    # output: a numpy array containing floats of the argmax index of the input: [2,1,...]
    # if offset is set to true, an offset will be applied to avoid overlapping ticks for graphs

    output_data = []
    for row in input_data:
        class_num = np.argmax(row)
        output_data.append(class_num.item())
    return np.array(output_data)


#  testing step
models.eval()
with torch.no_grad():
    if torch.cuda.is_available():
        x_test = x_test.cuda()
    if not torch.cuda.is_available():
        best_model = torch.load('outputs/final_model.pth', map_location=torch.device('cpu'))['model_state_dict']
    else:
        # cuda by default
        best_model = torch.load('outputs/final_model.pth')['model_state_dict']
    
    models.load_state_dict(best_model)
    out_data = models(x_test.float())

    # discrimination histogram graphing
    fig2, ax2 = plt.subplots(3)  # graph all 3 next to each other

    # copy tensor to local memory in order to use numpy
    out_data = out_data.cpu()

    # discrimination plot for class 0: interneurons
    class_0 = out_data[:, 0]
    c0_labeled0 = y_test[:, 0] == 0
    c0_labeled1 = y_test[:, 0] == 1
    c0_labeled2 = y_test[:, 0] == 2
    interneurons = class_0[c0_labeled0].detach().numpy().flatten()
    pyramidal = class_0[c0_labeled1].detach().numpy().flatten()
    unlabeled = class_0[c0_labeled2].detach().numpy().flatten()

    ax2[0].hist(interneurons, bins=8, range=[0, 1], label="interneurons", density=True, alpha=0.5)
    ax2[0].hist(pyramidal, bins=8, range=[0, 1], label="pyramidal", density=True, alpha=0.5)
    ax2[0].hist(unlabeled, bins=8, range=[0, 1], label="unlabeled", linewidth=1.7, histtype="step", density=True, alpha=0.5)
    ax2[0].set_title("Interneuron Discrimination Plot")
    ax2[0].legend(bbox_to_anchor=(1.5, 0.6))

    # discrimination plot for class 1: pyramidal cells
    class_1 = out_data[:, 1]
    c1_labeled0 = y_test[:, 1] == 0
    c1_labeled1 = y_test[:, 1] == 1
    c1_labeled2 = y_test[:, 1] == 2
    interneurons = class_1[c1_labeled0].detach().numpy().flatten()
    pyramidal = class_1[c1_labeled1].detach().numpy().flatten()
    unlabeled = class_1[c1_labeled2].detach().numpy().flatten()

    ax2[1].hist(interneurons, bins=8, range=[0, 1], label="interneurons", density=True, alpha=0.5)
    ax2[1].hist(pyramidal, bins=8, range=[0, 1], label="pyramidal", density=True, alpha=0.5)
    ax2[1].hist(unlabeled, bins=8, range=[0, 1], label="unlabeled", linewidth=1.7, histtype="step", density=True, alpha=0.5)
    ax2[1].set_title("Pyramidal Cell Discrimination Plot")
    ax2[1].legend(bbox_to_anchor=(1.5, 0.6))

    # discrimination plot for class 1: unlabeled cells
    class_2 = out_data[:, 2]
    c2_labeled0 = y_test[:, 2] == 0
    c2_labeled1 = y_test[:, 2] == 1
    c2_labeled2 = y_test[:, 2] == 2
    interneurons = class_2[c2_labeled0].detach().numpy().flatten()
    pyramidal = class_2[c2_labeled1].detach().numpy().flatten()
    unlabeled = class_2[c2_labeled2].detach().numpy().flatten()

    ax2[2].hist(interneurons, bins=8, range=[0, 1], label="interneurons", density=True, alpha=0.5)
    ax2[2].hist(pyramidal, bins=8, range=[0, 1], label="pyramidal", density=True, alpha=0.5)
    ax2[2].hist(unlabeled, bins=8, range=[0, 1], label="unlabeled", linewidth=1.7, histtype="step", density=True, alpha=0.5)
    ax2[2].set_title("Unlabeled Cell Discrimination Plot")
    ax2[2].legend(bbox_to_anchor=(1.5, 0.6))

    plt.tight_layout()
    plt.savefig("outputs/disc_hists.png")
    plt.show()

    # analysis that requires the labels to be one node (i.e. the index of the one hot encoded class) rather than 3
    predicted_class = decipher_class(out_data)
    expected_class = decipher_class(y_test)

    # test accuracy
    print("BALANCED ACCURACY:", met.balanced_accuracy_score(expected_class, predicted_class))

    correct_x = []
    wrong_x = []
    for i in range(predicted_class.shape[0]):
        if predicted_class[i] == expected_class[i]:
            correct_x.append(i)
        else:
            wrong_x.append(i)

    # raw, unbalanced accuracy
    accuracy = len(correct_x) / predicted_class.shape[0]
    print("test accuracy:", accuracy)

    # Discriminant Analysis
    y = expected_class
    target_names = ["interneuron", "pyramidal", "not identified"]
    
    # must be cpu in order to convert to numpy 
    # X = x_test.cpu()
    # analysis = discrim.LinearDiscriminantAnalysis()
    # data_plot = analysis.fit(X, y).transform(X)

    # create LDA plot
    # plt.figure()
    # colors = ["red", "blue", "green"]
    # lw = 2

    # plotting
    # for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    #     plt.scatter(data_plot[y == i, 0], data_plot[y == i, 1], alpha=0.8, color=color, label=target_name)
    # plt.legend(loc="best", shadow=False, scatterpoints=1)

    # plt.show()

    # confusion matrix
    confmat = met.confusion_matrix(expected_class, predicted_class)
    fig, ax = plt.subplots()
    print(confmat)

# plot the taylor coefficients after training
models.plot_taylor_coefficients(
    x_test.float(),
    considered_variables_idx=range(input_dims),
    variable_names=all_vars,
    derivation_order=1,
    path='outputs/ohe_coefficients.pdf'
)

# plot the saved checkpoints for the TCA model
models.plot_checkpoints(path='outputs/ohe_tc_training.pdf')


print('done!')
